{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "277fdef6",
   "metadata": {
    "_cell_guid": "fc951484-c7ed-44ea-8b7c-d4ee35164093",
    "_uuid": "38d8d4a2-e398-400a-92ee-b526d3ff9254",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-03-17T16:15:16.494613Z",
     "iopub.status.busy": "2024-03-17T16:15:16.493920Z",
     "iopub.status.idle": "2024-03-17T16:15:16.537505Z",
     "shell.execute_reply": "2024-03-17T16:15:16.536576Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.050706,
     "end_time": "2024-03-17T16:15:16.540327",
     "exception": false,
     "start_time": "2024-03-17T16:15:16.489621",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "class ReseauNeural(object):\n",
    "\n",
    "    def __init__(self, tailles):\n",
    "        self.num_layers = len(tailles)\n",
    "        self.tailles = tailles\n",
    "        self.biais = [np.random.randn(y, 1) for y in tailles[1:]]\n",
    "        self.poids = [np.random.randn(y, x) for x, y in zip(tailles[:-1], tailles[1:])]\n",
    "\n",
    "    def propagation_directe(self, a,f):\n",
    "        for b, w in zip(self.biais, self.poids):\n",
    "            a = f(np.dot(w, a)+b)\n",
    "        return a\n",
    "    \n",
    "    def Descente_gradient1(self, donnees_entrainement, taille_mini_lot, eta, check_lr,f, f_prime, donnees_test=None):\n",
    "        L = []\n",
    "        n_test = len(donnees_test)\n",
    "        L.append([0, self.calc_loss(donnees_test, f)])\n",
    "        n = len(donnees_entrainement)\n",
    "        mini_lots = [donnees_entrainement[k:k + taille_mini_lot] for k in range(0, n, taille_mini_lot)]\n",
    "        total_loss = 0  # Initialize total_loss for calculating average\n",
    "        total_examples_processed = 0  \n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        for i, mini_lot in enumerate(mini_lots, 1):\n",
    "            self.mettre_a_jour(mini_lot, eta, f, f_prime)\n",
    "            total_examples_processed += len(mini_lot)  # Increment total examples processed\n",
    "            total_loss += self.calc_loss(mini_lot, f)\n",
    "            \n",
    "            if total_examples_processed >= check_lr:\n",
    "                avg_loss = self.calc_loss(donnees_test, f)\n",
    "                print(f\"Examples {i * taille_mini_lot} - Average Test Loss: {avg_loss}\")\n",
    "\n",
    "                if avg_loss>=L[-1][-1]:\n",
    "                    eta=eta/10\n",
    "                    print(eta)\n",
    "                    check_lr\n",
    "                L.append([i * taille_mini_lot, avg_loss])\n",
    "                total_examples_processed = 0  # Reset total examples processed\n",
    "                \n",
    "        return L\n",
    "        \n",
    "    def Descente_gradient(self, donnees_entrainement, epochs, taille_mini_lot, eta, f, f_prime, donnees_test=None):\n",
    "        L = []\n",
    "        if donnees_test:\n",
    "            n_test = len(donnees_test)\n",
    "        n = len(donnees_entrainement)\n",
    "        for j in range(1, epochs + 1):\n",
    "            np.random.shuffle(donnees_entrainement)\n",
    "            mini_lots = [donnees_entrainement[k:k + taille_mini_lot] for k in range(0, n, taille_mini_lot)]\n",
    "            for i, mini_lot in enumerate(mini_lots, 1):\n",
    "                self.mettre_a_jour(mini_lot, eta, f, f_prime)\n",
    "\n",
    "            if donnees_test:\n",
    "                avg_loss = self.calc_loss(donnees_test, f)\n",
    "                print(f\"Epoch {j}: {avg_loss}\")\n",
    "                L.append([j, avg_loss])\n",
    "            else:\n",
    "                print(\"Epoch {0} is completed\".format(j))\n",
    "        clear_output(wait=True)  # Clear output after each epoch\n",
    "        return L\n",
    "\n",
    "    def calc_loss(self, donnees_test, f):\n",
    "        total_loss = 0\n",
    "        for x, y in donnees_test:\n",
    "            predicted = self.propagation_directe(x, f)\n",
    "            loss = self.loss(predicted, y)\n",
    "            total_loss += loss\n",
    "        avg_loss = total_loss / len(donnees_test)\n",
    "        return avg_loss[0][0]\n",
    "    \n",
    "    \n",
    "    def loss(self,predicted, actual):\n",
    "        return ((predicted- actual)**2)/2\n",
    "        \n",
    "\n",
    "    def mettre_a_jour(self, mini_lot, eta,f,f_prime):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biais]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.poids]\n",
    "        for x, y in mini_lot:\n",
    "            delta_nabla_b, delta_nabla_w = self.retroprop(x, y,f,f_prime)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.poids = [w-(eta/len(mini_lot))*nw for w, nw in zip(self.poids, nabla_w)]\n",
    "        self.biais = [b-(eta/len(mini_lot))*nb for b, nb in zip(self.biais, nabla_b)]\n",
    "\n",
    "\n",
    "    def retroprop(self, x, y,f,f_prime):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biais]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.poids]\n",
    "        activation = x\n",
    "        activations = [x] # liste pour stocker toutes les activations, couche par couche\n",
    "        zs = [] # liste pour stocker tous les vecteurs z, couche par couche\n",
    "        #passage en avant (calculer les activations correspondant à x)\n",
    "        for b, w in zip(self.biais, self.poids):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = f(z)\n",
    "            activations.append(activation)\n",
    "        # passage en arrière\n",
    "        delta = self.derivee_cout(activations[-1], y) * f_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = f_prime(z)\n",
    "            delta = np.dot(self.poids[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def derivee_cout(self, activation_sortie, y):\n",
    "        return (activation_sortie-y)\n",
    "\n",
    "    \n",
    "#fonction d'activation avec une sortie entre -1 et 1\n",
    "def sigmoid(z):\n",
    "    return 2.0 / (1.0 + np.exp(-z)) - 1.0\n",
    "def sigmoid_prime(z):\n",
    "    return 2.0 * np.exp(-z) / ((1.0 + np.exp(-z)) ** 2)\n",
    "def tanh(z):\n",
    "    return np.tanh(z)\n",
    "def tanh_prime(z):\n",
    "    return 1-tanh(z)**2\n",
    "def arctan(z):\n",
    "    return np.arctan(z)/(np.pi/2)\n",
    "def arctan_prime(z):\n",
    "    return (1/(1+z**2))/(np.pi/2)\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "def relu_prime(z):\n",
    "    return np.where(z > 0, 1, 0)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3.541033,
   "end_time": "2024-03-17T16:15:16.963594",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-03-17T16:15:13.422561",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
